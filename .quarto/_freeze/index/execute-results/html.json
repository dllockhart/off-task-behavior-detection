{
  "hash": "756580bba8425dc127894b94db83f1b4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Off-task Behavior Detection\"\nauthor: \"Demontray Lockhart\"\ndate: 12/23/2024\n---\n\n\n\n\n# Introduction\nThis project applies machine learning techniques to detect off-task behavior in students based on classroom data. \nIt leverages machine learning models to identify patterns in the data and uses cross-validation for model tuning.\n\n# Exploratory Data Analysis\nI import the necessary libraries to perform exploratory data analysis (EDA).\nNext, I load the dataset into a dataframe and generate descriptive statistics to understand the distribution and spread of the variables.\nI visualize the class distribution using a bar chart to identify any imbalance in the target variable.\nFinally, I create a correlation heatmap to examine relationships between the features and the target variable.\n\n::: {#45535827 .cell execution_count=1}\n``` {.python .cell-code}\n# Import necessary libraries for exploratory data analysis (EDA)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset into a pandas DataFrame\ndf = pd.read_csv(\"data/ca1-dataset.csv\")\n```\n:::\n\n\n::: {#b23b95b5 .cell execution_count=2}\n``` {.python .cell-code}\n# Display basic information about the dataset (column types, non-null counts)\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 763 entries, 0 to 762\nData columns (total 27 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   Unique-id             763 non-null    object \n 1   namea                 763 non-null    object \n 2   OffTask               763 non-null    object \n 3   Avgright              763 non-null    float64\n 4   Avgbug                763 non-null    float64\n 5   Avghelp               763 non-null    int64  \n 6   Avgchoice             763 non-null    int64  \n 7   Avgstring             763 non-null    int64  \n 8   Avgnumber             763 non-null    int64  \n 9   Avgpoint              763 non-null    int64  \n 10  Avgpchange            763 non-null    float64\n 11  Avgtime               763 non-null    float64\n 12  AvgtimeSDnormed       763 non-null    float64\n 13  Avgtimelast3SDnormed  763 non-null    float64\n 14  Avgtimelast5SDnormed  763 non-null    float64\n 15  Avgnotright           763 non-null    float64\n 16  Avghowmanywrong-up    763 non-null    float64\n 17  Avghelppct-up         763 non-null    int64  \n 18  Avgwrongpct-up        763 non-null    float64\n 19  Avgtimeperact-up      763 non-null    float64\n 20  AvgPrev3Count-up      763 non-null    float64\n 21  AvgPrev5Count-up      763 non-null    float64\n 22  Avgrecent8help        763 non-null    int64  \n 23  Avg recent5wrong      763 non-null    float64\n 24  Avgmanywrong-up       763 non-null    float64\n 25  AvgasymptoteA-up      763 non-null    int64  \n 26  AvgasymptoteB-up      763 non-null    int64  \ndtypes: float64(15), int64(9), object(3)\nmemory usage: 161.1+ KB\n```\n:::\n:::\n\n\n::: {#de25699c .cell execution_count=3}\n``` {.python .cell-code}\n# Generate descriptive statistics to understand the distribution and spread of the data\ndf.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Avgright</th>\n      <th>Avgbug</th>\n      <th>Avghelp</th>\n      <th>Avgchoice</th>\n      <th>Avgstring</th>\n      <th>Avgnumber</th>\n      <th>Avgpoint</th>\n      <th>Avgpchange</th>\n      <th>Avgtime</th>\n      <th>AvgtimeSDnormed</th>\n      <th>...</th>\n      <th>Avghelppct-up</th>\n      <th>Avgwrongpct-up</th>\n      <th>Avgtimeperact-up</th>\n      <th>AvgPrev3Count-up</th>\n      <th>AvgPrev5Count-up</th>\n      <th>Avgrecent8help</th>\n      <th>Avg recent5wrong</th>\n      <th>Avgmanywrong-up</th>\n      <th>AvgasymptoteA-up</th>\n      <th>AvgasymptoteB-up</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>763.000000</td>\n      <td>763.000000</td>\n      <td>763.0</td>\n      <td>763.0</td>\n      <td>763.0</td>\n      <td>763.0</td>\n      <td>763.0</td>\n      <td>763.000000</td>\n      <td>763.000000</td>\n      <td>763.000000</td>\n      <td>...</td>\n      <td>763.0</td>\n      <td>763.000000</td>\n      <td>763.000000</td>\n      <td>763.000000</td>\n      <td>763.000000</td>\n      <td>763.0</td>\n      <td>763.000000</td>\n      <td>763.000000</td>\n      <td>763.0</td>\n      <td>763.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.713072</td>\n      <td>0.055168</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.283065</td>\n      <td>14.117770</td>\n      <td>0.224450</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.483525</td>\n      <td>14.241320</td>\n      <td>0.446376</td>\n      <td>0.574707</td>\n      <td>0.0</td>\n      <td>0.914248</td>\n      <td>0.044288</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.394293</td>\n      <td>0.184326</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.386746</td>\n      <td>15.623914</td>\n      <td>1.317254</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>4.361797</td>\n      <td>10.366487</td>\n      <td>0.796067</td>\n      <td>1.097031</td>\n      <td>0.0</td>\n      <td>1.038397</td>\n      <td>0.179562</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>6.250000</td>\n      <td>-0.368185</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>7.838235</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>9.666667</td>\n      <td>-0.121190</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>11.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.750000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>16.000000</td>\n      <td>0.306074</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.750000</td>\n      <td>17.080128</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>1.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>205.000000</td>\n      <td>13.541537</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>47.666667</td>\n      <td>114.333333</td>\n      <td>3.000000</td>\n      <td>5.000000</td>\n      <td>0.0</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 24 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#b8855896 .cell execution_count=4}\n``` {.python .cell-code}\n# Visualize the distribution of the target variable ('OffTask') to check for class imbalance\nplt.figure(figsize=(6,4))\nsns.countplot(x='OffTask', data=df)\nplt.title(\"Class Distribution\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=519 height=376}\n:::\n:::\n\n\n::: {#b1fa5591 .cell execution_count=5}\n``` {.python .cell-code}\n# Calculate and visualize correlations between numerical features\nnumeric_df = df.select_dtypes(include=['float64', 'int64'])\nplt.figure(figsize=(10, 8))\nsns.heatmap(numeric_df.corr(), annot=False, cmap='coolwarm')\nplt.title(\"Feature Correlation Matrix\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=898 height=797}\n:::\n:::\n\n\nThe dataset consists of 763 entries and 27 columns, encompassing both numerical and categorical features.\n\nA review of the dataset reveals no missing values across columns, confirming data completeness. The dataset contains 15 float columns, 9 integer columns, and 3 object columns.\n\nDescriptive statistics highlight key patterns in the data:\n- 'Avgright' (average correctness) has a mean of 0.71, indicating students are often correct, with a standard deviation of 0.39.\n- 'Avgbug' (average errors) shows minimal occurrence, with a mean of 0.05.\n- Several features, such as 'Avghelp', 'Avgchoice', 'Avgstring', 'Avgnumber', 'Avgpoint', and 'AvgasymptoteA-up/B-up', consist entirely of zeros, suggesting they may be uninformative and could be removed during preprocessing.\n- The average time ('Avgtime') has a wide range, from -1 to 205, with a mean of 14.12 and notable variance (std = 15.62).\n- Features like 'AvgtimeSDnormed' and 'Avgpchange' exhibit both negative and positive values, which may indicate normalized or scaled data points.\n\nA bar chart of the target variable ('OffTask') highlights potential class imbalance, warranting further attention to address skewed distributions in model development.\n\nA correlation heatmap is generated to visualize relationships between numerical features. Metrics such as 'Avgtime', 'Avgtimeperact-up', and 'AvgPrev5Count-up' may reveal insights into patterns of off-task behavior.\n\nThe EDA process identifies redundant features with zero variance and confirms the dataset is well-structured for further preprocessing and model development. Addressing these redundant features will help streamline model performance and reduce unnecessary complexity.\n\n# Model Training & Evaluation\nI import the necessary libraries to train and evaluate multiple machine learning models. The dataset is prepared by removing non-predictive columns and eliminating features that contain only zero values.\n\nTo address class imbalance, I apply SMOTE (Synthetic Minority Over-sampling Technique) within each training fold, ensuring that synthetic samples are generated exclusively from the training data. This prevents data leakage and helps the models better recognize patterns in the minority class.\n\nI define three classifiers – Random Forest, Logistic Regression, and Support Vector Machine (SVM) – with class weights adjusted to mitigate the effects of class imbalance. For the Random Forest model, I implement a hyperparameter grid to fine-tune the model during training.\n\nI employ Stratified Group K-Fold cross-validation to maintain balanced class distributions across folds while accounting for group dependencies in the dataset. Within each fold, I apply GridSearchCV to optimize model parameters and evaluate performance on the test set.\n\nModel performance is assessed using the Cohen’s Kappa score and a classification report. The results for each classifier are aggregated, providing insights into overall performance across the cross-validation folds.\n\n::: {#942d5c72 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import StratifiedGroupKFold, GridSearchCV\nfrom sklearn.metrics import cohen_kappa_score, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nimport numpy as np\nimport pandas as pd\n\n# Data Preparation\nX = df.drop(columns=['Unique-id', 'OffTask', 'namea'])\ny = df['OffTask'].map({'N': 0, 'Y': 1})  # Map labels to binary (0, 1)\ngroups = df['namea']  # Grouping variable\n\n# Remove columns with all 0 values\nX = X.loc[:, (X != 0).any(axis=0)]\n\n# Define classifiers\nclassifiers = {\n    'Random Forest': RandomForestClassifier(class_weight='balanced'),\n    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced'),\n    'SVM': SVC(probability=True, class_weight='balanced')\n}\n\n# Hyperparameter grid for Random Forest\nparam_grid_rf = {\n    'classifier__n_estimators': [100, 200],\n    'classifier__max_depth': [10, 20, None],\n    'classifier__min_samples_split': [2, 5, 10],\n    'classifier__min_samples_leaf': [1, 2, 4]\n}\n\n# Stratified Group KFold\ngkf = StratifiedGroupKFold(n_splits=5)\nresults = {}\n\n# Model Training and Evaluation Loop\nfor name, classifier in classifiers.items():\n    pipeline = Pipeline(steps=[\n        ('classifier', classifier)\n    ])\n    \n    param_grid = param_grid_rf if name == 'Random Forest' else {}\n\n    # Store results\n    fold_scores = []\n    y_true_all, y_pred_all = [], []\n\n    for train_idx, test_idx in gkf.split(X, y, groups):\n        # Train-test split\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n\n        # Apply SMOTE only to the training data\n        smote = SMOTE(random_state=42)\n        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n\n        # Perform GridSearchCV\n        grid_search = GridSearchCV(pipeline, param_grid, cv=3)\n        grid_search.fit(X_train_res, y_train_res)\n\n        # Make predictions on the test set\n        y_pred = grid_search.best_estimator_.predict(X_test)\n\n        # Store results for evaluation\n        fold_scores.append(cohen_kappa_score(y_test, y_pred))\n        y_true_all.extend(y_test)\n        y_pred_all.extend(y_pred)\n    \n    # Final performance metrics\n    overall_kappa = cohen_kappa_score(y_true_all, y_pred_all)\n    overall_report = classification_report(y_true_all, y_pred_all)\n\n    # Save Results\n    results[name] = {\n        'mean_kappa': np.mean(fold_scores),\n        'overall_kappa': overall_kappa,\n        'report': overall_report\n    }\n\n    # Print Performance\n    print(f\"\\n{name} Results\")\n    print(f\"Mean Kappa Across Folds: {np.mean(fold_scores):.2f}\")\n    print(f\"Overall Kappa: {overall_kappa:.2f}\")\n    print(overall_report)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRandom Forest Results\nMean Kappa Across Folds: 0.17\nOverall Kappa: 0.16\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96       729\n           1       0.19      0.21      0.20        34\n\n    accuracy                           0.93       763\n   macro avg       0.58      0.58      0.58       763\nweighted avg       0.93      0.93      0.93       763\n\n\nLogistic Regression Results\nMean Kappa Across Folds: 0.12\nOverall Kappa: 0.11\n              precision    recall  f1-score   support\n\n           0       0.97      0.78      0.87       729\n           1       0.11      0.56      0.18        34\n\n    accuracy                           0.77       763\n   macro avg       0.54      0.67      0.52       763\nweighted avg       0.94      0.77      0.84       763\n\n\nSVM Results\nMean Kappa Across Folds: 0.18\nOverall Kappa: 0.18\n              precision    recall  f1-score   support\n\n           0       0.98      0.83      0.90       729\n           1       0.14      0.62      0.23        34\n\n    accuracy                           0.82       763\n   macro avg       0.56      0.72      0.57       763\nweighted avg       0.94      0.82      0.87       763\n\n```\n:::\n:::\n\n\n**Model Performance Overview:**\n\n- Random Forest:\n    - Achieves an overall accuracy of 93%, but performance on the minority class remains limited, with a recall of 18% and an f1-score of 0.18 for class 1.\n    - The overall kappa score is 0.14, indicating poor agreement between predictions and actual labels.\n\n- Logistic Regression:\n    - Yields an accuracy of 77%, with the minority class recall at 56%, reflecting the model's ability to identify positive cases. However, low precision for the minority class results in an f1-score of 0.18.\n    - The overall kappa score is 0.11, indicating slight agreement and limited predictive power for the minority class.\n\n- SVM:\n    - SVM demonstrates the highest minority class recall at 62% but maintains low precision, resulting in an f1-score of 0.23.\n    - With an overall accuracy of 82% and a kappa score of 0.18, SVM slightly outperforms the other models in balancing sensitivity to the minority class and overall accuracy.\n\n**Key Insights:**\nAcross all models, performance on the majority class (class 0) remains consistently high, while minority class detection (class 1) continues to present challenges. Despite applying SMOTE and adjusting class weights, low f1-scores and precision for the minority class indicate the need for:\n- Enhanced resampling techniques (e.g., ADASYN, Tomek links).\n- Feature engineering to improve class separability.\n- Exploration of ensemble methods or cost-sensitive learning to refine the model's ability to detect minority class instances.\n\nWhile SVM offers the best balance between recall and accuracy, further optimization is required to increase precision and overall model robustness when handling imbalanced datasets.\n\n# Visualizing the Peformance\nI import the necessary libraries to evaluate model performance through Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) metrics.\nA pipeline is created for each classifier, and the models are trained on the entire dataset without cross-validation to simplify visualization.\n\nThe models generate probability predictions for the positive class, which are used to calculate the ROC curve and AUC score.\nThe ROC curves for each classifier are plotted to compare performance, with a diagonal reference line representing random guessing.\n\nThe plot is finalized with titles, axis labels, and a legend to provide a clear visual comparison of model performance across different classifiers.\n\n::: {#4930a34b .cell execution_count=7}\n``` {.python .cell-code}\n## Visualize Performance\n\nfrom sklearn.metrics import roc_curve, auc, ConfusionMatrixDisplay\n\n\n# Visualize Performance\nplt.figure(figsize=(10, 7))\n\nfor name, classifier in classifiers.items():\n    # Create a simple pipeline with just the classifier\n    pipeline = Pipeline(steps=[\n        ('classifier', classifier)\n    ])\n    \n    # Fit the pipeline on the entire dataset (no cross-validation for visualization)\n    pipeline.fit(X, y)\n    \n    # Predict probabilities for the positive class\n    y_proba = pipeline.predict_proba(X)[:, 1]\n    \n    # Calculate ROC Curve and AUC\n    fpr, tpr, _ = roc_curve(y, y_proba)\n    roc_auc = auc(fpr, tpr)\n    \n    # Plot ROC Curve for each classifier\n    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n\n# Plot diagonal reference line\nplt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n\n# Finalize plot details\nplt.title('ROC Curve Comparison')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc='lower right')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=812 height=597}\n:::\n:::\n\n\nThe ROC curve highlights notable differences between the models:\n- Random Forest achieves a perfect AUC of 1.00, indicating overfitting or high reliance on the majority class.\n- SVM demonstrates strong performance with an AUC of 0.87, suggesting better generalization to minority class patterns.\n- Logistic Regression achieves an AUC of 0.84, reflecting slightly lower discrimination ability compared to SVM.\n\nThe visualization underscores the trade-off between sensitivity and specificity, with SVM and Logistic Regression showing promising performance, while Random Forest's perfect AUC suggests further investigation into overfitting.\n\n# Conclusion\nThe evaluation and visualization of three classifiers – Random Forest, Logistic Regression, and Support Vector Machine (SVM) – reveal significant performance differences, particularly in handling class imbalance.\n\n- Random Forest achieves the highest overall accuracy and a perfect AUC score, but this likely indicates overfitting or an inability to generalize to the minority class. Despite high overall accuracy, the model struggles to detect minority class instances (low recall and f1-score).\n- SVM offers the best balance between minority class recall (62%) and overall accuracy, with an AUC of 0.87 suggesting better generalization compared to Random Forest.\n- Logistic Regression provides moderate performance, with a recall of 56% for the minority class and an AUC of 0.84, indicating acceptable discrimination but lower precision.\n\nDespite SMOTE application and class weight adjustments, all models exhibit limited precision for the minority class, highlighting the need for further refinement. The ROC curve suggests SVM outperforms the other models in minority class detection, making it the most promising model for deployment.\n\n### Future Work\nTo enhance model performance, particularly for the minority class, future efforts will focus on addressing overfitting in Random Forest and improving recall-precision balance in all classifiers.\n\n**Key Areas for Improvement:**\n- Overfitting Mitigation (Random Forest):\n    - Regularize the Random Forest model by limiting the maximum depth and increasing minimum samples per leaf.\n    - Experiment with Balanced Random Forest to reduce reliance on the majority class.\n\n- Resampling Techniques:\n    - Continue exploring SMOTE variations (e.g., ADASYN) or hybrid approaches (combining oversampling and undersampling).\n    - Apply Tomek Links or Cluster Centroids to remove overlapping majority class samples.\n\n- Advanced Models and Ensemble Methods:\n    - Implement ensemble models such as XGBoost, LightGBM, or CatBoost, which handle imbalanced datasets more effectively.\n    - Investigate cost-sensitive learning to penalize misclassification of the minority class.\n\n- Threshold Tuning:\n    - Adjust classification thresholds to optimize the precision-recall trade-off, particularly for SVM and Logistic Regression.\n    - Use Precision-Recall Curves to select operating points that maximize minority class detection.\n\n- Feature Engineering:\n    - Derive new features that better separate minority class samples using domain knowledge.\n    - Perform dimensionality reduction (PCA, LDA) to highlight the most discriminative features.\n\n- Cross-Validation Refinement:\n    - Use nested cross-validation to tune hyperparameters and reduce overfitting.\n    - Apply stratified k-fold cross-validation with SMOTE applied within each fold to prevent data leakage.\n\nBy integrating these improvements, the goal is to enhance the recall and precision of minority class detection while ensuring high overall model accuracy and reduced overfitting, making the models more reliable for real-world deployment.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}